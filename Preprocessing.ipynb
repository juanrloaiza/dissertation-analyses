{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissertation Analytics\n",
    "## 1. Preprocessing\n",
    "\n",
    "This notebook preprocesses my dissertation _Emotions as functional kinds: a meta-theoretical approach to constructing scientific theories of emotions_ (HU Berlin) for further analysis. It uses NLTK to parse the text file in order to get a file we can analyze later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diss.txt') as f:\n",
    "    raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some replacements for better formatting. These are:\n",
    "* Changing line breaks (\\n) for spaces.\n",
    "* Removing indendation (e.g. \"oc- curred\").\n",
    "* Removing stylistic abbreviations: e.g., i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.replace('\\n', ' ')\n",
    "raw = raw.replace('- ', '')\n",
    "raw = raw.replace('e.g.', '')\n",
    "raw = raw.replace('i.e.', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.RegexpTokenizer('[A-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = nltk.sent_tokenize(raw.decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences does the dissertation have? How many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences: 4606\n",
      "# of words: 96588\n"
     ]
    }
   ],
   "source": [
    "print \"# of sentences: %s\" % len(raw_sentences)\n",
    "print \"# of words: %s\" % len(word_tokenizer.tokenize(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_by_word = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences_by_word.append(word_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tagged = []\n",
    "for sentence in sentences_by_word:\n",
    "    sent_tagged.append(nltk.pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_clean = []\n",
    "for sentence in sent_tagged:\n",
    "    sent_clean.append([word for word in sentence if word[0].lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diss_nostopwords.txt', 'w') as f:\n",
    "    f.write(str(sent_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLemmatizer(tagged_word):\n",
    "    word = tagged_word[0]\n",
    "    pos = tagged_word[1]\n",
    "    \n",
    "    if pos in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    elif pos in ['VB', 'VBN', 'VBZ', 'VBD', 'VBP', 'VBG']:\n",
    "        lemma = lemmatizer.lemmatize(word, pos = 'v')\n",
    "        \n",
    "    elif pos in ['JJ', 'JJR', 'JJS']:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
    "        \n",
    "    else:\n",
    "        lemma = False\n",
    "        \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = []\n",
    "for sentence in sent_clean:\n",
    "    for word in sentence:\n",
    "        lemma = customLemmatizer(word)\n",
    "        if lemma:\n",
    "            lemmatized_text.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diss_lemmatized.txt', 'w') as f:\n",
    "    f.write(str(lemmatized_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
